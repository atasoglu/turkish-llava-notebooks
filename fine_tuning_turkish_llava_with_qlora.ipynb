{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc2jIvWVyeTN"
      },
      "source": [
        "# Fine-tuning Turkish-LLaVA-v0.1 with QLoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62qM9Z4tHpzA"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "Welcome! This notebook will guide you through fine-tuning a multimodal [Turkish-LLaVA-v0.1](https://huggingface.co/ytu-ce-cosmos/Turkish-LLaVA-v0.1) model using QLoRA (Quantized Low-Rank Adaptation).\n",
        "\n",
        "What you'll learn:\n",
        "- Setting up your environment for LLaVA fine-tuning\n",
        "- Preparing and preprocessing a vision-language dataset\n",
        "- Using QLoRA for efficient training\n",
        "- Saving and sharing your fine-tuned model\n",
        "\n",
        "Requirements:\n",
        "- A GPU-enabled environment (NVIDIA A100 GPU recommended)\n",
        "- Basic familiarity with Python and Jupyter Notebooks\n",
        "- (Optional) A HuggingFace account for model sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K2ht6nTHpzB"
      },
      "source": [
        "## 2. Install Requirements\n",
        "\n",
        "We'll start by installing all necessary libraries: LLaVA, Hugging Face Transformers, PEFT (for LoRA), and more.\n",
        "\n",
        "⚠️ **Note:**\n",
        "- If you are running this on Google Colab, select a GPU runtime.\n",
        "- After installation, you may need to restart the notebook kernel.\n",
        "\n",
        "What is QLoRA?\n",
        "- QLoRA allows you to fine-tune large models efficiently by using quantized weights (4-bit) and low-rank adapters (LoRA), reducing memory usage and speeding up training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlZ1kitMAiz6"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip  # enable PEP 660 support\n",
        "!git clone https://github.com/haotian-liu/LLaVA.git\n",
        "!(cd LLaVA && pip install -e . && pip install -e \".[train]\")\n",
        "!pip install flash-attn==2.7.3 --no-build-isolation --no-cache-dir\n",
        "!pip install -U accelerate==0.34.2 peft==0.10.0 huggingface_hub datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqIuo9OIHpzC"
      },
      "source": [
        "## 3. Prepare the Model\n",
        "\n",
        "In this section, we:\n",
        "- Import necessary modules\n",
        "- Set up model, vision tower, and output directories\n",
        "- Define model, data, and training arguments\n",
        "\n",
        "**Tip:** You can change the model or vision tower by editing the variables below. Make sure they are compatible.\n",
        "\n",
        "What is a vision tower?\n",
        "- A vision tower is a neural network (often a CLIP model) that processes images and extracts features for the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edH1DgtlyeTU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from llava import conversation as conv\n",
        "from llava.train.train import ModelArguments, DataArguments, TrainingArguments\n",
        "\n",
        "\n",
        "vision_tower_name = \"openai/clip-vit-large-patch14-336\"\n",
        "model_path = \"ytu-ce-cosmos/Turkish-LLaVA-v0.1\"\n",
        "dataset_dir = \"llava_dataset\"\n",
        "output_dir = \"output\"\n",
        "\n",
        "data_args = DataArguments(\n",
        "    data_path=f\"{dataset_dir}/data.json\",\n",
        "    image_folder=f\"{dataset_dir}/images\",\n",
        "    lazy_preprocess=True,\n",
        ")\n",
        "\n",
        "model_args = ModelArguments(\n",
        "    model_name_or_path=model_path,\n",
        "    vision_tower=vision_tower_name,\n",
        "    mm_vision_select_layer=-2,\n",
        "    mm_use_im_start_end=False,\n",
        "    mm_use_im_patch_token=False,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    bf16=True,  # Use bf16 if your GPU supports it; else set to False or use fp16\n",
        "    per_device_train_batch_size=4,  # Increase for faster training if you have more GPU memory\n",
        "    gradient_accumulation_steps=4,  # Increase to simulate larger batch size without more memory\n",
        "    optim=\"adamw_8bit\",  # 8-bit optimizer saves memory; try \"adamw_torch\" for standard\n",
        "    learning_rate=1e-4,  # Lower for stability, higher for faster learning (e.g., 5e-5 to 2e-4)\n",
        "    warmup_ratio=0.03,  # Try 0.01–0.1; higher can help stabilize large models\n",
        "    weight_decay=0.01,  # Regularization; 0.01 is common, try 0.0–0.1\n",
        "    lr_scheduler_type=\"cosine\",  # \"linear\" or \"cosine\" are common; try both\n",
        "    # num_train_epochs=1,  # Use epochs or max_steps, not both; increase for more training\n",
        "    max_steps=1000,  # Increase for longer training; set to -1 to use num_train_epochs\n",
        "    logging_steps=5,  # Log more frequently for debugging, less for speed\n",
        "    save_strategy=\"steps\",  # \"epoch\" or \"steps\"; \"steps\" is good for long epochs\n",
        "    save_steps=100,  # Save more often to avoid losing progress, less for disk space\n",
        "    save_total_limit=5,  # Keep last N checkpoints; increase if you want more history\n",
        "    group_by_modality_length=True,  # Set False if you have OOM issues or single modality\n",
        "    mm_projector_lr=2e-5,  # Lower if overfitting, higher if underfitting vision branch\n",
        "    report_to=\"none\",  # Set to \"wandb\" or \"tensorboard\" for experiment tracking\n",
        ")\n",
        "\n",
        "# this is required for LLaMA-3 compatibilty\n",
        "system_prompt = \"Sen yardımsever bir asistansın.\"\n",
        "conv.default_conversation = conv.Conversation(\n",
        "    system=f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}\",\n",
        "    roles=(\n",
        "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "    ),\n",
        "    version=\"llama3\",\n",
        "    messages=[],\n",
        "    offset=0,\n",
        "    sep_style=conv.SeparatorStyle.MPT,\n",
        "    sep=\"<|eot_id|>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dTsfEbPyeTV"
      },
      "source": [
        "## 4. Load & Quantize the Model (4-bit)\n",
        "\n",
        "Here, we load the pretrained model and quantize it to 4-bit precision using BitsAndBytes.\n",
        "\n",
        "Why quantize?\n",
        "- Reduces memory usage and allows you to train larger models on smaller GPUs.\n",
        "- 4-bit quantization is a good balance between efficiency and performance.\n",
        "\n",
        "**Note:** If you encounter CUDA or memory errors, try reducing the batch size or using a smaller model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJLZKzJuyeTV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llava.model.builder import load_pretrained_model\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
        "    model_path,\n",
        "    None,\n",
        "    \"llava_llama\",\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.torch_dtype = torch.bfloat16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GI9TjExyeTW"
      },
      "source": [
        "## 5. Set Up LoRA (Low-Rank Adaptation)\n",
        "\n",
        "LoRA is a parameter-efficient fine-tuning method. Instead of updating all model weights, it adds small trainable matrices (adapters) to certain layers.\n",
        "\n",
        "Why use LoRA?\n",
        "- Dramatically reduces the number of trainable parameters\n",
        "- Makes fine-tuning feasible on consumer hardware\n",
        "\n",
        "What you can change: The LoRA rank (r), alpha, and dropout. Higher rank = more capacity, but more memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSEVww5wyeTW"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from llava.train.train import find_all_linear_names\n",
        "\n",
        "# Set the LoRA parameters below\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=find_all_linear_names(model),\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(\n",
        "    model,\n",
        "    use_gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "if hasattr(model, \"enable_input_require_grads\"):\n",
        "    model.enable_input_require_grads()\n",
        "else:\n",
        "\n",
        "    def make_inputs_require_grad(module, input, output):\n",
        "        output.requires_grad_(True)\n",
        "\n",
        "    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm4SlH7MyeTX"
      },
      "source": [
        "## 6. Align Vision Tower and Tokenizer\n",
        "\n",
        "This step ensures that the image processor and tokenizer are correctly set up and aligned with the model.\n",
        "\n",
        "Why is this important?\n",
        "- Multimodal models need to process both text and images.\n",
        "- Proper alignment ensures that images and text are handled consistently during training.\n",
        "\n",
        "Advanced: If you want to freeze or tune specific parts of the model (like the vision tower), you can adjust the flags here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1PPVSBYyeTY"
      },
      "outputs": [],
      "source": [
        "vision_tower = model.get_vision_tower()\n",
        "vision_tower.to(dtype=torch.bfloat16, device=\"cuda\")\n",
        "\n",
        "data_args.image_processor = vision_tower.image_processor\n",
        "data_args.is_multimodal = True\n",
        "\n",
        "model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
        "model.config.tokenizer_padding_side = tokenizer.padding_side\n",
        "model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
        "\n",
        "model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
        "if model_args.tune_mm_mlp_adapter:\n",
        "    model.requires_grad_(False)\n",
        "    for p in model.get_model().mm_projector.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
        "if training_args.freeze_mm_mlp_adapter:\n",
        "    for p in model.get_model().mm_projector.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "model.get_model().mm_projector.to(dtype=torch.bfloat16, device=\"cuda\")\n",
        "\n",
        "model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
        "model.config.mm_projector_lr = training_args.mm_projector_lr\n",
        "training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
        "model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
        "model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjYl-GltyeTZ"
      },
      "source": [
        "## 7. Set LoRA Layer Data Types\n",
        "\n",
        "For best performance and stability, we set the data types (dtypes) of LoRA and normalization layers.\n",
        "\n",
        "Why?\n",
        "- Some layers work better in float32, others in bfloat16.\n",
        "- This helps prevent numerical issues during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tcs_aDryeTZ"
      },
      "outputs": [],
      "source": [
        "from peft.tuners.lora import LoraLayer\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, LoraLayer):\n",
        "        module = module.to(torch.bfloat16)\n",
        "    if 'norm' in name:\n",
        "        module = module.to(torch.float32)\n",
        "    if 'lm_head' in name or 'embed_tokens' in name:\n",
        "        if hasattr(module, 'weight'):\n",
        "            if module.weight.dtype == torch.float32:\n",
        "                module = module.to(torch.bfloat16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2CafSbeyeTZ"
      },
      "source": [
        "## 8. Prepare the Dataset\n",
        "\n",
        "We will download and preprocess a Turkish vision-language dataset.\n",
        "\n",
        "What happens here:\n",
        "- Download the dataset from Hugging Face\n",
        "- Preprocess images and captions into the format required by LLaVA\n",
        "- Save the processed data for training\n",
        "\n",
        "**Tip:** You can use your own dataset by changing the dataset path and preprocessing logic. Make sure to split your data into training and validation sets for best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAOQk1YwHpzG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "def prepare_dataset_path(dataset_dir: Union[str, Path]):\n",
        "    dataset_dir = Path(dataset_dir)\n",
        "    if not dataset_dir.exists():\n",
        "        dataset_dir.mkdir(parents=True)\n",
        "    if not (dataset_dir / \"images\").exists():\n",
        "        (dataset_dir / \"images\").mkdir(parents=True)\n",
        "\n",
        "\n",
        "def preprocess(batch: list[dict], batch_size: int, user_prompt: str):\n",
        "    batch[\"json\"] = []\n",
        "    for i in range(batch_size):\n",
        "        img = f\"{batch['imgid'][i]}.jpg\"\n",
        "        batch[\"image\"][i].save(f\"llava_dataset/images/{img}\", format=\"JPEG\")\n",
        "        batch[\"json\"].append(\n",
        "            {\n",
        "                \"id\": batch[\"imgid\"][i],\n",
        "                \"image\": img,\n",
        "                \"conversations\": [\n",
        "                    {\"from\": \"human\", \"value\": f\"<image>\\n{user_prompt}\"},\n",
        "                    {\"from\": \"gpt\", \"value\": batch[\"detailed_caption\"][i]},\n",
        "                ],\n",
        "            },\n",
        "        )\n",
        "    return batch\n",
        "\n",
        "# prepare dataset path first\n",
        "prepare_dataset_path(dataset_dir)\n",
        "\n",
        "batch_size = 1000\n",
        "dataset_dir = \"llava_dataset\"\n",
        "user_prompt = \"Görüntüyü detaylı olarak açıkla.\"\n",
        "ds = load_dataset(\"atasoglu/flickr8k-turkish-detailed-captions\", split=\"train\")\n",
        "ds = ds.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    fn_kwargs=dict(batch_size=batch_size, user_prompt=user_prompt),\n",
        ")\n",
        "with open(f\"{dataset_dir}/data.json\", \"w\") as f:\n",
        "    f.write(json.dumps(ds[\"json\"], indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0Cio6XCyeTZ"
      },
      "source": [
        "## 9. Fix End-of-Sequence (EOS) Token Issue\n",
        "\n",
        "Some models may not handle the EOS token correctly. Here, we patch the library function to ensure the EOS token is added.\n",
        "\n",
        "**Note:** This is a temporary workaround. If the library updates, this patch may break. Always check for official fixes or updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmH_7ENByeTa"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import llava.train.train as llava_train\n",
        "\n",
        "@functools.wraps(llava_train._add_speaker_and_signal)\n",
        "def patched_fn(header, source, get_conversation=True):\n",
        "    EOS_TOKEN = \"<|eot_id|>\" # for Llama-3\n",
        "    conversation = header\n",
        "    for sentence in source:\n",
        "        from_str = sentence[\"from\"]\n",
        "        if from_str.lower() == \"human\":\n",
        "            from_str = conv.default_conversation.roles[0]\n",
        "        elif from_str.lower() == \"gpt\":\n",
        "            from_str = conv.default_conversation.roles[1]\n",
        "        else:\n",
        "            from_str = 'unknown'\n",
        "        sentence[\"value\"] = (EOS_TOKEN + from_str + sentence[\"value\"])\n",
        "        if get_conversation:\n",
        "            conversation += sentence[\"value\"]\n",
        "    conversation += EOS_TOKEN\n",
        "    return conversation\n",
        "\n",
        "llava_train._add_speaker_and_signal = patched_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUNsKIBmyeTa"
      },
      "source": [
        "## 10. Prepare the Data Module\n",
        "\n",
        "We create a data module that handles tokenization and batching for training.\n",
        "\n",
        "Why?\n",
        "- Data modules make it easy to manage datasets and data loaders.\n",
        "- Ensures consistent preprocessing and batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woc2YJViyeTa"
      },
      "outputs": [],
      "source": [
        "from llava.train.train import make_supervised_data_module\n",
        "\n",
        "data_module = make_supervised_data_module(\n",
        "    tokenizer=tokenizer,\n",
        "    data_args=data_args,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q5xV1BWyeTa"
      },
      "source": [
        "## 11. Inspect Tokenized Input (Optional)\n",
        "\n",
        "Let's look at a sample of the tokenized input to make sure everything is working as expected.\n",
        "\n",
        "**Tip:** If the decoded text looks strange, check your preprocessing and tokenizer settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZy-EPbeyeTa"
      },
      "outputs": [],
      "source": [
        "from llava.constants import IMAGE_TOKEN_INDEX\n",
        "example_text_input = data_module[\"train_dataset\"][0][\"input_ids\"].unsqueeze(0)\n",
        "# ignore image token since that is related with image processor\n",
        "decoded = tokenizer.decode(example_text_input[example_text_input != IMAGE_TOKEN_INDEX])\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1nlla-AyeTa"
      },
      "source": [
        "## 12. Start Training!\n",
        "\n",
        "Now we're ready to train the model.\n",
        "\n",
        "Tips for training:\n",
        "- Monitor GPU memory usage and training loss.\n",
        "- Adjust batch size or gradient accumulation steps if you run out of memory.\n",
        "- Training can take several hours depending on your hardware and dataset size.\n",
        "\n",
        "**Advanced:** For best results, use a validation set and monitor validation loss to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s66x2JuyeTb"
      },
      "outputs": [],
      "source": [
        "from llava.train.llava_trainer import LLaVATrainer\n",
        "\n",
        "trainer = LLaVATrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    **data_module,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg6e0QfVWUPQ"
      },
      "source": [
        "## 13. Save and Share Your Model\n",
        "\n",
        "After training, you can save your model and push it to the Hugging Face Hub for sharing and future use.\n",
        "\n",
        "What is the Hugging Face Hub?\n",
        "- A platform for sharing models, datasets, and demos.\n",
        "- You can create a free account at https://huggingface.co\n",
        "\n",
        "**Note:** You will be prompted to log in to your Hugging Face account. Make sure to save all components: model, tokenizer, and image processor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOa4Zm80HpzI"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdrflZnMOqaw"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Load & merge model with LoRA adapter and convert to bfloat16\n",
        "model = PeftModel.from_pretrained(model_path, output_dir)\n",
        "model = model.merge_and_unload().bfloat16()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdn9CXSJpcLm"
      },
      "outputs": [],
      "source": [
        "merged_path = \"merged_output\"\n",
        "repo_id = \"atasoglu/Turkish-LLaVA-v0.1-ft\"\n",
        "push_to_hub = True\n",
        "\n",
        "# Save (and push to hub) the model\n",
        "model.save_pretrained(\n",
        "    merged_path,\n",
        "    repo_id=repo_id,\n",
        "    push_to_hub=push_to_hub,\n",
        ")\n",
        "tokenizer.save_pretrained(\n",
        "    merged_path,\n",
        "    repo_id=repo_id,\n",
        "    push_to_hub=push_to_hub,\n",
        ")\n",
        "image_processor.save_pretrained(\n",
        "    merged_path,\n",
        "    repo_id=repo_id,\n",
        "    push_to_hub=push_to_hub,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRWEaAc1HpzI"
      },
      "source": [
        "## 14. Additional Tips and Best Practices\n",
        "\n",
        "- Set random seeds for reproducibility (see torch.manual_seed, numpy, etc.).\n",
        "- Use a validation split to monitor overfitting.\n",
        "- Visualize training progress (e.g., with TensorBoard).\n",
        "- Check hardware compatibility (CUDA, bfloat16 support, etc.).\n",
        "- Consult official documentation for LLaVA, Hugging Face, and PEFT for updates and troubleshooting.\n",
        "\n",
        "Happy fine-tuning! 🚀"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
