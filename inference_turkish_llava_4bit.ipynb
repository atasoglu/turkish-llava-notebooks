{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9b80c48e",
      "metadata": {
        "id": "9b80c48e"
      },
      "source": [
        "# Inference Turkish-LLaVA-v0.1 with 4bit Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0146699a",
      "metadata": {
        "id": "0146699a"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "Welcome! This notebook will guide you through running inference with the multimodal [Turkish-LLaVA-v0.1](https://huggingface.co/ytu-ce-cosmos/Turkish-LLaVA-v0.1) model using 4-bit quantization for efficient memory usage.\n",
        "\n",
        "What you'll learn:\n",
        "- Setting up your environment for LLaVA inference\n",
        "- Loading and quantizing the Turkish-LLaVA model\n",
        "- Preparing and processing images and prompts\n",
        "- Running inference and interpreting the results\n",
        "\n",
        "Requirements:\n",
        "- A GPU-enabled environment (NVIDIA A100 or similar recommended)\n",
        "- Basic familiarity with Python and Jupyter Notebooks\n",
        "\n",
        "> **Tip:** This notebook is designed for easy use on Google Colab, Kaggle, or your own GPU machine. If youâ€™re new to multimodal models, donâ€™t worryâ€”each step is explained in detail!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0323b0b6",
      "metadata": {
        "id": "0323b0b6"
      },
      "source": [
        "## 2. Install Requirements\n",
        "\n",
        "We start by installing all necessary libraries: LLaVA, Hugging Face Transformers, PEFT, and more.\n",
        "\n",
        "âš ï¸ **Notes:**\n",
        "- If you are running this on Google Colab, make sure to select a GPU runtime.\n",
        "- After installation, you may need to restart the notebook kernel.\n",
        "- The \"flash-attn\" library is required for efficient attention computation.\n",
        "- \"BitsAndBytes\" enables 4-bit quantization for large models.\n",
        "\n",
        "> If you encounter installation issues, try upgrading pip or restarting the runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c63a5ff",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "5c63a5ff"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip  # enable PEP 660 support\n",
        "!git clone https://github.com/haotian-liu/LLaVA.git\n",
        "!(cd LLaVA && pip install -e . && pip install -e \".[train]\")\n",
        "!pip install flash-attn==2.7.3 --no-build-isolation --no-cache-dir\n",
        "!pip install -U accelerate==0.34.2 peft==0.10.0 huggingface_hub datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7318d321",
      "metadata": {
        "id": "7318d321"
      },
      "source": [
        "## 3. Define Helper Functions\n",
        "\n",
        "We define utility functions for:\n",
        "- Downloading images from a URL\n",
        "- Preprocessing prompts for the LLaVA model\n",
        "- Running inference with the model\n",
        "\n",
        "**Why?**\n",
        "- These helpers make the code modular and reusable.\n",
        "- The prompt formatting is crucial for multimodal models like LLaVA.\n",
        "\n",
        "> **Tip:** You can modify the prompt templates to suit your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1537741",
      "metadata": {
        "id": "d1537741"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from llava.mm_utils import process_images, tokenizer_image_token\n",
        "\n",
        "\n",
        "def download_image(url):\n",
        "    content = requests.get(url).content\n",
        "    return Image.open(BytesIO(content))\n",
        "\n",
        "\n",
        "def preprocess_prompt(system_prompt, user_prompt):\n",
        "    return (\n",
        "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "        f\"{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "        f\"<image>\\n{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def inference(model, tokenizer, image_processor, image, prompt, generation_config):\n",
        "    input_ids = (\n",
        "        tokenizer_image_token(\n",
        "            prompt,\n",
        "            tokenizer,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        .unsqueeze(0)\n",
        "        .cuda()\n",
        "    )\n",
        "    image_tensor = (\n",
        "        process_images(\n",
        "            [image],\n",
        "            image_processor,\n",
        "            model.config,\n",
        "        )\n",
        "        .to(model.device, dtype=torch.float16)\n",
        "    )\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        images=image_tensor,\n",
        "        image_sizes=[image.size],\n",
        "        **generation_config,\n",
        "    )\n",
        "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96ff6203",
      "metadata": {
        "id": "96ff6203"
      },
      "source": [
        "## 4. Download and Prepare the Model\n",
        "\n",
        "We now download the pretrained Turkish-LLaVA model and set up 4-bit quantization using BitsAndBytes.\n",
        "\n",
        "**Why quantize?**\n",
        "- 4-bit quantization dramatically reduces memory usage, making it possible to run large models on consumer GPUs.\n",
        "- The \"nf4\" quantization type is recommended for best performance.\n",
        "\n",
        "**Tips:**\n",
        "- If you encounter CUDA or memory errors, try reducing batch size or using a smaller model.\n",
        "- Make sure your GPU supports bfloat16 (A100, RTX 30xx, etc.), or change to float16 if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c6c839",
      "metadata": {
        "id": "a4c6c839"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.utils import disable_torch_init\n",
        "\n",
        "model_path = \"ytu-ce-cosmos/Turkish-LLaVA-v0.1\"\n",
        "\n",
        "# apply 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "disable_torch_init() # for inference\n",
        "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
        "    model_path,\n",
        "    None,\n",
        "    \"llava_llama\",\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1e66a0",
      "metadata": {
        "id": "cc1e66a0"
      },
      "source": [
        "## 5. Prepare the Input (Image and Prompt)\n",
        "\n",
        "Let's prepare an example image and a prompt for the model.\n",
        "\n",
        "- We use a sample image from the Hugging Face datasets.\n",
        "- The prompt consists of a system message (defining the assistant's behavior) and a user message (the actual question or instruction).\n",
        "\n",
        "**Tips:**\n",
        "- You can change the image URL to try your own images.\n",
        "- Modify the user prompt to ask different questions about the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95fcf243",
      "metadata": {
        "id": "95fcf243"
      },
      "outputs": [],
      "source": [
        "# get image from URL address\n",
        "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-im-captioning.jpg\"\n",
        "image = download_image(image_url)\n",
        "\n",
        "# create a prompt with system and user messages\n",
        "system_prompt = \"Sen yardÄ±msever bir asistansÄ±n.\"\n",
        "user_prompt = \"GÃ¶rÃ¼ntÃ¼yÃ¼ detaylÄ± olarak aÃ§Ä±kla.\"\n",
        "prompt = preprocess_prompt(system_prompt, user_prompt)\n",
        "display(image)\n",
        "print(\"Prompt:\", prompt, sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a6c42a",
      "metadata": {
        "id": "d2a6c42a"
      },
      "source": [
        "## 5. Run Inference\n",
        "\n",
        "Now, let's run the model on the prepared image and prompt.\n",
        "\n",
        "- The model will generate a detailed caption or answer about the image.\n",
        "- You can adjust the \"generation_config\" (e.g., max_new_tokens) for longer or shorter outputs.\n",
        "\n",
        "> **Tip:** If you get CUDA out-of-memory errors, try reducing max_new_tokens or use a smaller image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26db62c6",
      "metadata": {
        "id": "26db62c6"
      },
      "outputs": [],
      "source": [
        "# define generation config\n",
        "generation_config = dict(\n",
        "    do_sample=False,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "\n",
        "# run inference\n",
        "outputs = inference(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    image_processor,\n",
        "    image,\n",
        "    prompt,\n",
        "    generation_config,\n",
        ")\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1cefa26",
      "metadata": {
        "id": "b1cefa26"
      },
      "source": [
        "## 6. Additional Tips and Troubleshooting\n",
        "\n",
        "- If you want to use your own images, upload them and use `Image.open('your_image.jpg')`.\n",
        "- For different tasks (e.g., VQA, conversation), adjust the prompt template accordingly.\n",
        "- If you encounter errors related to CUDA, try restarting the runtime or reducing memory usage.\n",
        "- For more advanced usage, see the [LLaVA GitHub repository](https://github.com/haotian-liu/LLaVA) and [Hugging Face documentation](https://huggingface.co/docs/transformers/main/en/model_doc/llava).\n",
        "\n",
        "Happy inferencing! ðŸš€"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}